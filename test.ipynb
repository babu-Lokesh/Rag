{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aeef85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, logging\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e4db3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from langchain import hub\n",
    "MODEL = \"gpt-4\"\n",
    "OpenAi_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "TEMPERATURE = 0\n",
    "logger = logging.getLogger(__name__)\n",
    "PROMPT = hub.pull(\"rlm/rag-prompt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdc4894",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def get_llm() -> ChatOpenAI:\n",
    "    return ChatOpenAI(model = MODEL,temperature=TEMPERATURE,api_key = OpenAi_api_key )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dd1f66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ragtool,answer billing tool,technical query,escalte to human,general tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d122afa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "@tool\n",
    "def answer_using_rag(query: str) -> dict:\n",
    "    \"\"\"\n",
    "    Use the knowledge base (via RAG) to answer the user's query.\n",
    "\n",
    "    Falls back to a general response if the RAG result is insufficient.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(VECTORSTORE_DIR):\n",
    "            logger.info(\"Vector store not found. Triggering update.\")\n",
    "            update_vector_store()\n",
    "\n",
    "        vector_store = FAISS.load_local(\n",
    "            VECTORSTORE_DIR,\n",
    "            embeddings=get_embadings(),\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "\n",
    "        results = vector_store.similarity_search(query=query, k=3)\n",
    "        context = \" \".join([doc.page_content for doc in results]) if results else \"\"\n",
    "        logger.info(f\"Retrieved {len(results)} relevant documents for query: '{query}'\")\n",
    "\n",
    "        if not context:\n",
    "            logger.warning(\"No relevant context found. Falling back to general query.\")\n",
    "            return general_query.invoke({\"query\": query})\n",
    "\n",
    "        chain = PROMPT | get_llm() | StrOutputParser()\n",
    "        result = chain.invoke({\"context\": context, \"question\": query})\n",
    "\n",
    "        if \"does not provide information\" in result.lower() or len(result.strip()) < 10:\n",
    "            logger.warning(\"RAG output is vague. Falling back to general query.\")\n",
    "            return general_query.invoke({\"query\": query})\n",
    "\n",
    "        return {\"input\": query, \"output\": result}\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.exception(\"RAG-based answering failed.\")\n",
    "        return {\"input\": query, \"output\": f\"An error occurred: {str(e)}\"}\n",
    "\n",
    "@tool\n",
    "def answer_billing_query(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Answer billing related query.\n",
    "\n",
    "    \"\"\"\n",
    "    promt = \"you are an helpfull asistant to give billing answers to the customers\"\n",
    "    promt1 = ChatPromptTemplate.from_messages(\n",
    "       [ (\"system\",promt)]\n",
    "    )\n",
    "\n",
    "    llm = get_llm()\n",
    "    chain = promt1 | llm | StrOutputParser()\n",
    "    result = chain.invoke({\"question\": query})\n",
    "    return {\"input\": query, \"output\": f\"{result}\"}\n",
    "\n",
    "@tool\n",
    "def answer_technical_query(query: str) -> str:\n",
    "    \"\"\"Answer technical support related query.\"\"\"\n",
    "    return {\"input\": query, \"output\": f\"This is a technical support response to: {query}\"}\n",
    "\n",
    "@tool\n",
    "def general_query(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a generic response when no specific tool applies.\n",
    "    \"\"\"\n",
    "    parser = StrOutputParser()\n",
    "    chain = get_llm() | parser\n",
    "    output = chain.invoke(query)\n",
    "    return {\"input\": query, \"output\": f\"This is a general response to: '{output}'\"}\n",
    "\n",
    "\n",
    "@tool\n",
    "def escalate_to_human(query: str) -> str:\n",
    "    \"\"\" \n",
    "    Escalating to live agent\n",
    "    \"\"\"\n",
    "    return {\"input\": query, \"output\": f\"Escalating to human support for query: '{query}'\"}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ae0e83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "\n",
    "\n",
    "def get_tools():\n",
    "    \"\"\"\n",
    "    Returns a list of tools available to the support agent.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        answer_using_rag,\n",
    "        answer_billing_query,\n",
    "        answer_technical_query,\n",
    "        general_query,\n",
    "        escalate_to_human,\n",
    "        ]\n",
    "\n",
    "def get_agent():\n",
    "    \"\"\"\n",
    "    Initializes and returns a LangChain agent configured with tools and an LLM.\n",
    "    Returns: LangChain agent instance\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tools = get_tools()\n",
    "        llm = get_llm()\n",
    "\n",
    "        #logger.info(\"Initializing support agent with tools and LLM.\")\n",
    "        agent = initialize_agent(\n",
    "            tools=tools,\n",
    "            llm=llm,\n",
    "            agent=AgentType.OPENAI_FUNCTIONS,\n",
    "            verbose=True\n",
    "        )\n",
    "        return agent\n",
    "\n",
    "    except Exception as e:\n",
    "        #logger.exception(\"Failed to initialize agent.\")\n",
    "        raise RuntimeError(f\"Agent initialization error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6c495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cahin = promt|llm|ouputparser\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3c9ce4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = get_agent()\n",
    "#agent.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19043d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \" what is by billing date\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe91a858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `answer_billing_query` with `{'query': 'What is my billing date?'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m{'input': 'What is my billing date?', 'output': \"Customer: I have a question about my bill. Why is it higher than usual this month?\\n\\nAssistant: I'm here to help! There could be several reasons why your bill is higher this month. It could be due to additional services or products you've purchased, a rate increase, or possibly some error. Could you please provide me with more details or your account number so I can look into it for you?\"}\u001b[0m\u001b[32;1m\u001b[1;3mI'm here to help! There could be several reasons why your bill is higher this month. It could be due to additional services or products you've purchased, a rate increase, or possibly some error. Could you please provide me with more details or your account number so I can look into it for you?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = agent.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fdbfd7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 = \" can you connect me to an human agent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "856903be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `escalate_to_human` with `{'query': 'can you connect me to an human agent'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m{'input': 'can you connect me to an human agent', 'output': \"Escalating to human support for query: 'can you connect me to an human agent'\"}\u001b[0m\u001b[32;1m\u001b[1;3mSure, I'm escalating your request to a human agent. They will be with you shortly.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = agent.invoke(question1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bab2dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
